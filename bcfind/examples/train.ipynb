{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">BCFind training</h1>\n",
    "This notebook gives an example on how to train BCFind by using modules and classes provided by the library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import lmdb\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from bcfind.data import TrainingDataset\n",
    "from bcfind.models import ResUNet\n",
    "from bcfind.losses import FramedCrossentropy3D\n",
    "from bcfind.metrics import Precision, Recall, F1\n",
    "from bcfind.localizers import BlobDoG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNet\n",
    "\n",
    "### 1. Dataset\n",
    "\n",
    "- **Paired lists of input/target paths**\n",
    "\n",
    "> Input files must be .tiff or .tif \\\n",
    "> Target files must be generated by Vaa3D (.marker) or 3D-Slicer (.json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_my_data = \"My_Data\"\n",
    "tiff_dir = f\"{path_to_my_data}/Tiff_files/Train\"\n",
    "gt_dir = f\"{path_to_my_data}/GT_files/Train\"\n",
    "\n",
    "tiff_files = [f\"{tiff_dir}/{fname}\" for fname in os.listdir(tiff_dir)]\n",
    "gt_files = [f\"{gt_dir}/{fname}.marker\" for fname in os.listdir(tiff_dir)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Data augmentation**\n",
    "\n",
    "> BCFind-v2 offers a set of operations for data augmentation which can be selected from the following dictionary. \\\n",
    "> If `augmentations` is set to None, no data augmentation will be performed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentations = {\n",
    "    \"gamma\": {\"param_range\": [0.9, 1.1]},\n",
    "    # 'contrast': {'param_range': [1., 3.]},\n",
    "    \"brightness\": {\"param_range\": [-0.06, 0.06]},\n",
    "    # 'zoom': {'param_range': [1.0, 1.1], 'order':1},\n",
    "    \"blur\": {\"param_range\": [0.0, 0.3]},\n",
    "    \"noise\": {\"param_range\": [0.0, 0.03]},\n",
    "    # 'rotation': {'param_range': [0., 270.], 'axes': [-2, -1]},\n",
    "    \"flip\": {\"axes\": [-2]},\n",
    "}\n",
    "\n",
    "augmentations_probs = [\n",
    "    0.3,\n",
    "] * len(augmentations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Pre-processing**\n",
    "\n",
    "> `clip` threshold set a ceiling value for the inputs \\\n",
    "> `center` subtracts a specific value to all input pixels \\\n",
    "> `scale` divides all input pixels by a specific value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = {\n",
    "    \"clip\": \"bit\",\n",
    "    \"clip_value\": 15,  # clip can be ['bit', 'constant', 'quantile', None]\n",
    "    \"center\": None,\n",
    "    \"center_value\": None,  # center can be one of ['constant', 'min', 'mean', null]\n",
    "    \"scale\": \"bit\",\n",
    "    \"scale_value\": 15,  # scale can be one of ['constant', 'bit', 'max', 'std', null]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **TrainingDataset class**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "voxel_resolution = (2.0, 0.65, 0.65)\n",
    "input_shape = (80, 240, 240)\n",
    "\n",
    "train_data = TrainingDataset(\n",
    "    tiff_files,\n",
    "    gt_files,\n",
    "    batch_size,\n",
    "    voxel_resolution,\n",
    "    input_shape,\n",
    "    augmentations,\n",
    "    augmentations_probs,\n",
    "    **preprocessing\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model\n",
    "\n",
    "- **Build model architecture**\n",
    "\n",
    "> Building with an input shape of (None, None, None, None, 1) is useful for shape flexibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResUNet(\n",
    "    n_blocks=4,\n",
    "    n_filters=16,\n",
    "    k_size=(3, 5, 5),\n",
    "    k_stride=(2, 2, 2),\n",
    "    dropout=None,\n",
    "    regularizer=None,\n",
    ")\n",
    "model.build((None, None, None, None, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Model compile**\n",
    "\n",
    "> Mandatory definitions:\n",
    ">\n",
    "> - `loss`\n",
    "> - `optimizer`\n",
    "> - `learning-rate`\n",
    ">\n",
    "> Optional definitions:\n",
    ">\n",
    "> - `metrics`\n",
    "> - `learning-rate scheduler`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_border = (3, 9, 9)\n",
    "learning_rate = 0.01\n",
    "\n",
    "loss = FramedCrossentropy3D(exclude_border, input_shape, from_logits=True)\n",
    "\n",
    "prec = Precision(0.006, input_shape, exclude_border, from_logits=True)\n",
    "rec = Recall(0.006, input_shape, exclude_border, from_logits=True)\n",
    "f1 = F1(0.006, input_shape, exclude_border, from_logits=True)\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "    learning_rate,\n",
    "    first_decay_steps=100,\n",
    "    t_mul=2,\n",
    "    m_mul=0.8,\n",
    "    alpha=1e-4,\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(\n",
    "    lr_schedule, momentum=0.9, nesterov=True, weight_decay=7e-4\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=[prec, rec, f1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Callbacks**\n",
    "\n",
    "> `ModelCheckpoint` takes care of saving the model each time the loss value improves\n",
    ">\n",
    "> `TensorBoard` monitors the loss and metrics during training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_my_exp = \"My_Exp\"\n",
    "unet_checkpoint_dir = f\"{path_to_my_exp}/UNet_checkpoints\"\n",
    "tensorboard_dir = f\"{path_to_my_exp}/UNet_tensorboard\"\n",
    "\n",
    "MC_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    f\"{unet_checkpoint_dir}/model.tf\",\n",
    "    initial_value_threshold=0.1,\n",
    "    save_best_only=True,\n",
    "    save_format=\"tf\",\n",
    "    save_freq=\"epoch\",\n",
    "    monitor=\"loss\",\n",
    "    mode=\"min\",\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "TB_callback = tf.keras.callbacks.TensorBoard(\n",
    "    tensorboard_dir,\n",
    "    profile_batch=0,\n",
    "    write_graph=True,\n",
    ")\n",
    "\n",
    "callbacks = [MC_callback, TB_callback]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Model training**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_data,\n",
    "    epochs=3000,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=None,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blob Detection with DoG\n",
    "\n",
    "### 1. Input/target pairs\n",
    "\n",
    "> `dog_inputs`: \\\n",
    "> The blob detector takes UNet predictions as inputs. We therefore need to save all UNet predictions in a iterable. \\\n",
    "> Since all of them can be too big to fit into memory an lmdb database can be used.\n",
    ">\n",
    "> `dog_targets`: \\\n",
    "> DoG targets are the arrays of true coordinates: a list of them is usually fine to fit into memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bcfind.utils.models import predict\n",
    "from bcfind.utils.data import get_input_tf, get_gt_as_numpy\n",
    "\n",
    "\n",
    "max_input_shape = [160, 480, 480]\n",
    "lmdb_dir = f\"{path_to_my_exp}/UNet_pred_train_lmdb\"\n",
    "\n",
    "n = len(tiff_files)\n",
    "nbytes = np.prod(max_input_shape) * 1  # 4 bytes for float32: 1 byte for uint8\n",
    "\n",
    "# UNet predictions\n",
    "print(f\"Saving U-Net predictions in {lmdb_dir}\")\n",
    "db = lmdb.open(lmdb_dir, map_size=n * nbytes * 10)\n",
    "with db.begin(write=True) as fx:\n",
    "    for i, tiff_file in enumerate(tiff_files):\n",
    "        print(f\"\\nUnet prediction on file {i+1}/{len(tiff_files)}\")\n",
    "\n",
    "        x = get_input_tf(tiff_file, **preprocessing)\n",
    "        pred = predict(x, model)\n",
    "        pred = tf.sigmoid(tf.squeeze(pred)).numpy()\n",
    "        pred = (pred * 255).astype(\"uint8\")\n",
    "\n",
    "        fname = tiff_file.split(\"/\")[-1]\n",
    "        fx.put(key=fname.encode(), value=pickle.dumps(pred))\n",
    "\n",
    "db.close()\n",
    "dog_inputs = lmdb.open(lmdb_dir, readonly=True)\n",
    "\n",
    "# True cell coordinates\n",
    "dog_targets = []\n",
    "for gt_file in gt_files:\n",
    "    print(f\"Loading file {gt_file}\")\n",
    "    y = get_gt_as_numpy(gt_file)\n",
    "    dog_targets.append(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Hyper-parameter tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim = 3\n",
    "max_match_dist = 10\n",
    "iterations = 50\n",
    "dog_checkpoint_dir = f\"{path_to_my_exp}/DoG_checkpoints\"\n",
    "\n",
    "dog = BlobDoG(3, voxel_resolution, exclude_border)\n",
    "\n",
    "with dog_inputs.begin() as fx:\n",
    "    X = fx.cursor()\n",
    "    dog.fit(\n",
    "        X=X,\n",
    "        Y=dog_targets,\n",
    "        max_match_dist=max_match_dist,\n",
    "        n_iter=iterations,\n",
    "        checkpoint_dir=dog_checkpoint_dir,\n",
    "        n_cpu=5,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
